{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5/pSIR6dYG0mmltihSf9B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deeprajbasu/MachineLearningCore/blob/main/sequential_data_rnn%2BLSTM%2Bgru.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sequential data\n",
        "# examples, audio, video, date wise temperatiure data, stock price data, language etc\n",
        "\n",
        "# context, memory. "
      ],
      "metadata": {
        "id": "bIhtqHbWo4Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recurrent Neural Network \n",
        "\n",
        "# Hidden state, which remembers some information about a sequence\n",
        "\n",
        "# current state : ht = f(ht-1,xt)\n",
        "# ht -> current state\n",
        "# ht-1 -> previous state\n",
        "# xt -> input state\n",
        "\n",
        "\n",
        "#output \n",
        "# yt = Why . ht\n",
        "# Yt -> output\n",
        "# Why -> weight at output layer\n",
        "# ht -> current state\n",
        "\n",
        "\n",
        "\n",
        "# An RNN remembers each and every piece of information through time. \n",
        "# ie. every sequence input influences state\n",
        "\n",
        "# backpropagation is unstable as calculating derivatives is expensive\n",
        "\n",
        "\n",
        "# context vs causation"
      ],
      "metadata": {
        "id": "wC_kbd-gozuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aOvV-h8TIyEP"
      },
      "outputs": [],
      "source": [
        "# Long Short-Term Memory Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# maintaining context for Sequential data\n",
        "\n",
        "# previous state\n",
        "\n",
        "# rnn causes unstable gradient descent because ineffectient 'context' or 'memory'\n",
        "\n",
        "#lstm:\n",
        "# keep or forget data depending on the needs of the network.\n",
        "\n",
        "# network can choose to forget data whenever information is not needed"
      ],
      "metadata": {
        "id": "dCwM9b0xLp7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#forget gate function kt\n",
        "\n",
        "𝐾𝑡=𝜎(𝑊𝑘×[𝑆𝑡−1,𝑥𝑡]+𝐵𝑘)\n",
        "\n",
        "𝑂𝑙𝑑𝑡=𝐾𝑡×𝑂𝑙𝑑𝑡−1\n",
        "\n",
        "-----------------------\n",
        "\n",
        "𝑥𝑡 current input features\n",
        "\n",
        "𝑆𝑡−1 previous state\n",
        "\n",
        "𝑊𝑘 forget gate weights\n",
        "\n",
        "𝐵𝑘 forget gate bias\n",
        "\n",
        "𝑂𝑙𝑑𝑡= previous state in network based on sigmoid output from forget gate"
      ],
      "metadata": {
        "id": "FwwkzD9GMsBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gated Recurrent Unit\n",
        "\n",
        "#reset gate - short term memery\n",
        "#update gate  - long term memory \n",
        "\n"
      ],
      "metadata": {
        "id": "hOrOPMhirf4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "_xYX2jpfMncb"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Natural language processing with lstm \n",
        "\n",
        "# find out what is the next word\n",
        "\n",
        "# sequential language context constructed by previous words.\n",
        "\n",
        "# LSTM has capacity to maintain the model's state for over one thousand time steps"
      ],
      "metadata": {
        "id": "dVC-pt3dOPXm"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word embedding\n",
        "\n",
        "# systematic way to represent strings as vectors \n",
        "\n",
        "# 𝑉𝑒𝑐(\"𝐸𝑥𝑎𝑚𝑝𝑙𝑒\")=[0.02,0.00,0.00,0.92,0.30,…]"
      ],
      "metadata": {
        "id": "-rXtkpEVOCLG"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate natural language with lstm"
      ],
      "metadata": {
        "id": "UiNrPcWvZk4m"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "\n",
        "print(f'Length of text: {len(text)} characters')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_f23cUZOJc5",
        "outputId": "bb7ff303-ece5-44d5-9241-526e5b06e5f5"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[0:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "NBsJ40tFUEl3",
        "outputId": "87d1b43a-2043-4864-ff12-ca37144bbf08"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary from training data for embeddings\n",
        "vocab = sorted(set(text))\n",
        "vocab[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHI0EIQ3P2U4",
        "outputId": "0b3558a9-7890-4f41-8e31-d7c25c482cdd"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3']"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vectorize string input\n",
        "\n",
        "#reconstruct strings from vector for text generation \n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "\n",
        "\n",
        "example_texts = ['Citizen', 'resolved','e']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-l-vzqeQBxy",
        "outputId": "824357dc-550b-4dfc-f476-bab604467eff"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[16, 48, 59, 48, 65, 44, 53], [57, 44, 58, 54, 51, 61, 44, 43], [44]]>"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "print(text_from_ids(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-1o4dpaYdgL",
        "outputId": "370182f4-06cc-4d67-d0b0-30949b08b14d"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([b'Citizen' b'resolved' b'e'], shape=(3,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Data\n",
        "\n",
        "#id dataset from all of text\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di4J8-MAYszO",
        "outputId": "0ea64a17-5714-4308-cb1f-456cdef6aca8"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "seq_length = 24\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "#print sequence of training data \n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kej1oLVOX_GN",
        "outputId": "8d997178-8ac0-49d5-e98d-fda34c34d569"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we '\n",
            "b'proceed any further, hear'\n",
            "b' me speak.\\n\\nAll:\\nSpeak, s'\n",
            "b'peak.\\n\\nFirst Citizen:\\nYou'\n",
            "b' are all resolved rather '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model needs to predict next character\n",
        "# input data and target data example\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "print(split_input_target(list(\"12345\")))\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "#X,y for each sequences\n",
        "for input_example, target_example in dataset.take(3):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw54J12VZwbW",
        "outputId": "64dfcef6-5379-4dc4-8ebd-4c64a4a81341"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['1', '2', '3', '4'], ['2', '3', '4', '5'])\n",
            "Input : b'First Citizen:\\nBefore we'\n",
            "Target: b'irst Citizen:\\nBefore we '\n",
            "Input : b'proceed any further, hea'\n",
            "Target: b'roceed any further, hear'\n",
            "Input : b' me speak.\\n\\nAll:\\nSpeak, '\n",
            "Target: b'me speak.\\n\\nAll:\\nSpeak, s'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle sequences\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "# dataset = (\n",
        "#     dataset\n",
        "#     .shuffle(BUFFER_SIZE)\n",
        "#     .batch(BATCH_SIZE, drop_remainder=True)\n",
        "#     .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "for input_example, target_example in dataset.take(2):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwuZBoT6axFC",
        "outputId": "645e262f-1210-42fc-9ab9-195bc0eee262"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : [b'nce; our\\nsufferance is a' b'eanness that\\nafflicts us'\n",
            " b'be done: away, away!\\n\\nSe' b'feits on would relieve u'\n",
            " b': if they\\nwould yield us' b'\\n\\nFirst Citizen:\\nVery we'\n",
            " b\"ore talking on't; let it\" b'rt, but that he pays him'\n",
            " b'cond Citizen:\\nNay, but s' b' good.\\nWhat authority su'\n",
            " b' in hunger for bread, no' b\"\\nAll:\\nWe know't, we know\"\n",
            " b' me speak.\\n\\nAll:\\nSpeak, ' b'articularise their abund'\n",
            " b'elf with being proud.\\n\\nS' b\" kill him, and we'll hav\"\n",
            " b'y he did it to\\nplease hi' b'Second Citizen:\\nWould yo'\n",
            " b'First Citizen:\\nFirst, yo' b'l; and could be content '\n",
            " b'virtue.\\n\\nSecond Citizen:' b'ight guess they relieved'\n",
            " b', is as an\\ninventory to ' b'very dog to the commonal'\n",
            " b'ge this with\\nour pikes, ' b'en of accusations;\\nhe ha'\n",
            " b'od citizens.\\n\\nFirst Citi' b'u, what he hath done fam'\n",
            " b'risen: why stay we prati' b' corn at our own price.\\n'\n",
            " b'All:\\nCome, come.\\n\\nFirst ' b\"ther side o' the city\\nis\"\n",
            " b' the object of our miser' b'ond Citizen:\\nOne word, g'\n",
            " b'\\n\\nFirst Citizen:\\nIf I mu' b'First Citizen:\\nBefore we'\n",
            " b' mother and to be partly' b'itizen:\\nSoft! who comes '\n",
            " b'nd: though soft-conscien' b'proud; which he\\nis, even'\n",
            " b' citizens, the patrician' b'say it was for his count'\n",
            " b'o give him good\\nreport f' b'ief enemy to the people.'\n",
            " b're we become rakes: for ' b'but the superfluity, whi'\n",
            " b'nk we are too dear: the ' b'eak not maliciously.\\n\\nFi'\n",
            " b' are all resolved rather' b'he gods know I\\nspeak thi'\n",
            " b'us humanely;\\nbut they th' b'till the altitude of his'\n",
            " b'to die than to famish?\\n\\n' b\":\\nHe's one honest enough\"\n",
            " b' proceed especially agai' b'proceed any further, hea'\n",
            " b'h faults, with surplus, ' b'What he cannot help in h'\n",
            " b'ice in him. You must in ' b'eak, I pray you.\\n\\nFirst '\n",
            " b'st Caius Marcius?\\n\\nAll:\\n' b'now we\\nhave strong arms '\n",
            " b'you undo yourselves?\\n\\nFi' b'riends, most charitable ']\n",
            "Target: [b'ce; our\\nsufferance is a ' b'anness that\\nafflicts us,'\n",
            " b'e done: away, away!\\n\\nSec' b'eits on would relieve us'\n",
            " b' if they\\nwould yield us ' b'\\nFirst Citizen:\\nVery wel'\n",
            " b\"re talking on't; let it \" b't, but that he pays hims'\n",
            " b'ond Citizen:\\nNay, but sp' b'good.\\nWhat authority sur'\n",
            " b'in hunger for bread, not' b\"All:\\nWe know't, we know'\"\n",
            " b'me speak.\\n\\nAll:\\nSpeak, s' b'rticularise their abunda'\n",
            " b'lf with being proud.\\n\\nSe' b\"kill him, and we'll have\"\n",
            " b' he did it to\\nplease his' b'econd Citizen:\\nWould you'\n",
            " b'irst Citizen:\\nFirst, you' b'; and could be content t'\n",
            " b'irtue.\\n\\nSecond Citizen:\\n' b'ght guess they relieved '\n",
            " b' is as an\\ninventory to p' b'ery dog to the commonalt'\n",
            " b'e this with\\nour pikes, e' b'n of accusations;\\nhe hat'\n",
            " b'd citizens.\\n\\nFirst Citiz' b', what he hath done famo'\n",
            " b'isen: why stay we pratin' b'corn at our own price.\\nI'\n",
            " b'll:\\nCome, come.\\n\\nFirst C' b\"her side o' the city\\nis \"\n",
            " b'the object of our misery' b'nd Citizen:\\nOne word, go'\n",
            " b'\\nFirst Citizen:\\nIf I mus' b'irst Citizen:\\nBefore we '\n",
            " b'mother and to be partly ' b'tizen:\\nSoft! who comes h'\n",
            " b'd: though soft-conscienc' b'roud; which he\\nis, even '\n",
            " b'citizens, the patricians' b'ay it was for his countr'\n",
            " b' give him good\\nreport fo' b'ef enemy to the people.\\n'\n",
            " b'e we become rakes: for t' b'ut the superfluity, whil'\n",
            " b'k we are too dear: the l' b'ak not maliciously.\\n\\nFir'\n",
            " b'are all resolved rather ' b'e gods know I\\nspeak this'\n",
            " b's humanely;\\nbut they thi' b'ill the altitude of his '\n",
            " b'o die than to famish?\\n\\nA' b\"\\nHe's one honest enough:\"\n",
            " b'proceed especially again' b'roceed any further, hear'\n",
            " b' faults, with surplus, t' b'hat he cannot help in hi'\n",
            " b'ce in him. You must in n' b'ak, I pray you.\\n\\nFirst C'\n",
            " b't Caius Marcius?\\n\\nAll:\\nA' b'ow we\\nhave strong arms t'\n",
            " b'ou undo yourselves?\\n\\nFir' b'iends, most charitable c']\n",
            "Input : [b'g here? to the Capitol!\\n' b' that hath always loved\\n'\n",
            " b'f you. For your wants,\\nY' b\"nd to do,\\nwhich now we'l\"\n",
            " b'll:\\nResolved. resolved.\\n' b'say poor\\nsuitors have st'\n",
            " b'oo.\\n\\nMENENIUS:\\nWhy, mast' b'st Citizen:\\nWe cannot, s'\n",
            " b'y.\\n\\nSecond Citizen:\\nCons' b's nature, you account a\\n'\n",
            " b'it takes, cracking ten t' b'ong breaths: they shall '\n",
            " b't shouts are these? The ' b'has done for his country'\n",
            " b'iment. For the dearth,\\nT' b' know Caius Marcius is c'\n",
            " b's, my countrymen, in han' b'der you what services he'\n",
            " b't.\\n\\nFirst Citizen:\\nLet u' b\"lms o' the state, who ca\"\n",
            " b'are\\nHave the patricians ' b'o way say he is covetous'\n",
            " b't not, I need not be bar' b'e at the heaven with you'\n",
            " b' in thirst for revenge.\\n' b' they have\\nhad inkling t'\n",
            " b'ur suffering in this dea' b\"s't a verdict?\\n\\nAll:\\nNo \"\n",
            " b'o tire in repetition.\\nWh' b'rs, my good friends, min'\n",
            " b' usurers; repeal daily a' b'\\n\\nMENENIUS:\\nI tell you, '\n",
            " b'cts for usury, to\\nsuppor' b'usly, he did\\nit to that '\n",
            " b'nd their store-houses\\ncr' b's, make it, and\\nYour kne'\n",
            " b'peak.\\n\\nFirst Citizen:\\nYo' b' staves as lift them\\nAga'\n",
            " b'provide more\\npiercing st' b'y wholesome act\\nestablis'\n",
            " b'he people.\\n\\nFirst Citize' b'nsported by calamity\\nThi'\n",
            " b' honest neighbours,\\nWill' b'thy Menenius Agrippa; on'\n",
            " b'\\nBut, since it serves my' b'itizen:\\nOur business is '\n",
            " b'ot unknown to the senate' b'en:\\nWe are accounted poo'\n",
            " b'st Citizen:\\nI say unto y' b'll you\\nA pretty tale: it'\n",
            " b'may be you have heard it' b\"hey ne'er cared for us\\ny\"\n",
            " b\": but, an 't please\\nyou,\" b'mmed with grain; make ed'\n",
            " b'so!\\n\\nMENENIUS:\\nWhat work' b\"gainst him first: he's a\"\n",
            " b' would all the rest were' b'e for us! True, indeed! '\n",
            " b't: suffer us to famish, ' b' the wars eat us not up,'\n",
            " b' was a time when all the' b'rous malicious,\\nOr be ac'\n",
            " b'll hear it, sir: yet you' b\"they will; and\\nthere's a\"]\n",
            "Target: [b' here? to the Capitol!\\n\\n' b'that hath always loved\\nt'\n",
            " b' you. For your wants,\\nYo' b\"d to do,\\nwhich now we'll\"\n",
            " b'l:\\nResolved. resolved.\\n\\n' b'ay poor\\nsuitors have str'\n",
            " b'o.\\n\\nMENENIUS:\\nWhy, maste' b't Citizen:\\nWe cannot, si'\n",
            " b'.\\n\\nSecond Citizen:\\nConsi' b' nature, you account a\\nv'\n",
            " b't takes, cracking ten th' b'ng breaths: they shall k'\n",
            " b' shouts are these? The o' b'as done for his country?'\n",
            " b'ment. For the dearth,\\nTh' b'know Caius Marcius is ch'\n",
            " b', my countrymen, in hand' b'er you what services he '\n",
            " b'.\\n\\nFirst Citizen:\\nLet us' b\"ms o' the state, who car\"\n",
            " b're\\nHave the patricians o' b' way say he is covetous.'\n",
            " b' not, I need not be barr' b' at the heaven with your'\n",
            " b'in thirst for revenge.\\n\\n' b'they have\\nhad inkling th'\n",
            " b'r suffering in this dear' b\"'t a verdict?\\n\\nAll:\\nNo m\"\n",
            " b' tire in repetition.\\nWha' b's, my good friends, mine'\n",
            " b'usurers; repeal daily an' b'\\nMENENIUS:\\nI tell you, f'\n",
            " b'ts for usury, to\\nsupport' b'sly, he did\\nit to that e'\n",
            " b'd their store-houses\\ncra' b', make it, and\\nYour knee'\n",
            " b'eak.\\n\\nFirst Citizen:\\nYou' b'staves as lift them\\nAgai'\n",
            " b'rovide more\\npiercing sta' b' wholesome act\\nestablish'\n",
            " b'e people.\\n\\nFirst Citizen' b'sported by calamity\\nThit'\n",
            " b'honest neighbours,\\nWill ' b'hy Menenius Agrippa; one'\n",
            " b'But, since it serves my ' b'tizen:\\nOur business is n'\n",
            " b't unknown to the senate;' b'n:\\nWe are accounted poor'\n",
            " b't Citizen:\\nI say unto yo' b'l you\\nA pretty tale: it '\n",
            " b'ay be you have heard it;' b\"ey ne'er cared for us\\nye\"\n",
            " b\" but, an 't please\\nyou, \" b'med with grain; make edi'\n",
            " b\"o!\\n\\nMENENIUS:\\nWhat work'\" b\"ainst him first: he's a \"\n",
            " b'would all the rest were ' b' for us! True, indeed! T'\n",
            " b': suffer us to famish, a' b'the wars eat us not up, '\n",
            " b'was a time when all the ' b'ous malicious,\\nOr be acc'\n",
            " b'l hear it, sir: yet you ' b\"hey will; and\\nthere's al\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rznClsspXIif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build model\n",
        "\n",
        "\n",
        "# embedding layer (input layer map character id to vector, trainable)\n",
        "# lstm layer (rnn layer)\n",
        "\n",
        "# dense layer (output layer, likeleyhood of each character in vocab)"
      ],
      "metadata": {
        "id": "mILW5ETzbZ_O"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "num_steps = 20"
      ],
      "metadata": {
        "id": "gvnHrkw8ba-8"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the model \n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    \n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "4uEe4gS3eEEu"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n"
      ],
      "metadata": {
        "id": "mTNsat2ZfsY3"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSLJGsi5fvMa",
        "outputId": "569169aa-bf55-41ec-e930-c66f28687fb6"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 24, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oa4OlOhjaHb",
        "outputId": "da5af8d8-ea46-485b-8d63-988f50f993c1"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_14 (Embedding)    multiple                  16896     \n",
            "                                                                 \n",
            " gru_3 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_11 (Dense)            multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model \n",
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "LsoY2WK3oCa1",
        "outputId": "527cdf40-6470-4737-969e-9dfcb6385319"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "697/697 [==============================] - 1005s 1s/step - loss: 2.0416\n",
            "Epoch 2/20\n",
            "  8/697 [..............................] - ETA: 21:01 - loss: 1.8674"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-166-421f17580e2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n"
      ],
      "metadata": {
        "id": "gYVZnHDsnJs0"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n"
      ],
      "metadata": {
        "id": "wCdZrcsvsyHy"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i6Uc8kas0bO",
        "outputId": "1ab29951-f44c-410a-91ce-3f623cca395a"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "First this disstowth cize him ither\n",
            "Was common thy gildand, what eschize, this\n",
            "Liss here for't, cationf ground.\n",
            "\n",
            "SlAvION:\n",
            "Comine,\n",
            "He crosh a vingue\n",
            "Turf they? that\n",
            "is very all,\n",
            "And, one awry\n",
            "thou dose fulseloved-caunion say is no coussion that the eries 'tis yaur that withar in fhere?\n",
            "\n",
            "SO:\n",
            "Houts, but the vilit toll, readinty\n",
            "I'll gentle brend, it woned, curizen,\n",
            "'Tis thur,--\n",
            "\n",
            "SOBASTIAN:\n",
            "Your crising heart. \n",
            "MARINAN:\n",
            "I talizes veny froth your dutlts. Of first forgunellict\n",
            "of not is as that confeart, stors;\n",
            "Be it betend let my art again: Vencusinits,\n",
            "And hold thou sitner.\n",
            "\n",
            "\n",
            "\n",
            "BAPNISABIIO:\n",
            "Nrant age as,\n",
            "For groul hastard never ging not; that for\n",
            "Ay anl-my tuncys,\n",
            "Sir, my madistresess the mage?\n",
            "\n",
            "LO:\n",
            "Shourth yeers looks, with rich falsely somes, thy bribon:\n",
            "Mone the ispast.\n",
            "\n",
            "CAN?\n",
            "\n",
            "CANII:\n",
            "WhO cross Crainy Well\n",
            "sonst, wilt thene crecuized:\n",
            "Which you confenter.\n",
            "\n",
            "SEBASTIAN:\n",
            "Which I make pidgivences fentensio have\n",
            "selve that youl, and praifors untogain?\n",
            "\n",
            "ASTANII:\n",
            "In whiliraft them, this which me \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.680391788482666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RlktE0oUs6hN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}